---
title: Microservices on a budget: ignoring platform complexity
date: "2019-05-18"
---

== Microservices failures: Ignoring platform complexity

I think there is a common misconception about microservices out there.

The popular opinion is that microservices supposed to be the cure for all of our problems.
But I say that by themselves they can cure only half of our problems.
You are suppose to combine them with the latest DevBizSecDbaQaOps practices in order to transform your company into the ultimate buzzword producing killing machinegun and achieve Continuous Deployment nirvana.

But seriously.

Countless of blogs promised us that microservices will be the the ultimate cure.
They were supposed to enable companies to perform gazillions deployments per day, scale the system up to infinity while saving money and reducing the codebase complexity at the same time.
But few asked what is the price for all of this?

Many forgets that microservices architecture shifts the complexity from the code to the platform.

== Complexity

> Complex - many parts with intricate arrangements, organized (easy) or disorganized (hard)

> Complicated - hard to understand regardless of the complexity

We can describe any software system on how complicated and complex it is.
System can be made less complicated and complex by removing pieces of it.
When splitting a system into pieces it may become less complicated but the complexity will increase.
Thankfully splitting can be done in such a way that we end up with an organized complexity which is easy to understand.

Microservices may reduce complexity from the code but just the opposite happens to the platform.  

== Platform definition

Software which is not a shelfware will eventually end up running on a platform.
It's all the things which enables the software to perform.
When platform fails your software fails.  

My definition of any software platform includes:

* *Platform topology* - existing infrastructure
* *Services* - executable units of software
** *Core services* - executables required to fulfil business needs
** *Support services* - executables monitoring core services and the platform itself
** *Service Orchestration* - executables assigning which service goes where, also responsible for scaling and destroying services
* *Service Configuration* - configuration which changes depending on the environment
* *Service Secrets* - configuration which should not be source controlled
* *Service Discovery* - detection of services
* *Service Mesh* - delivery of requests through a topology

Additionally we have things which describes the platform setup and deployment procedures:

* *Platform definition* - procedures (hopefuly in code) describing in details the infrastructure
* *Deployment pipelines* - procedures (hopefully in code) describing the deployment process

Every platform has a lifecycle:

* Day 1

    There is nothing.
    Infrastructure gets created and software gets deployment for the first time.

* Day 2+

  Infrastructure gets extended or updated.
  Software gets redeployed.

Project crippling problems arise not on day 1, but on day 2 onwards.
Platform is like a gifted puppy. Upfront cost may be zero but when puppy grows and multiplies someone will have to maintain that.
Eventually a badly designed and poorly maintained platform will behave oddly in the most unexpected ways up to a point where instead of fixing the current one it's cheaper to just create a new one.

== Platform as an afterthought

First I would like to point out that if you haven't noticed yet this post is my own private opinion on the subject.

There are two cases when starting with microservices architecture:

1. Starting fresh - what we call a greenfield project.
2. Migrating from a monolith by extracting bits from it.

Unfortunately both cases usually suffer from the same symptoms:

> Platform is an afterthought.

== Paying attention to details

In my opinion the most common mistakes when "designing" platform for a microservices are:

1. Lack of monitoring.
2. Making artifacts mutable.
3. Not following the https://12factor.net/["twelve factors"] rules.
4. Designing pipelines with no automated rollback strategy.
5. Not changing the tools with scale.
6.

== Monitoring, observability and debuggability

Monitoring is gathering and displaying data so it can be analyzed.  
To monitor a system it must be observable.

> If you are observable I can understand you.

The tools and techniques needed to analyze a system composed of couple services vs hundredths of services are vastly different.
Where one can manage to manually gather and sift through metrics for few services, doing so for dozens is not sustainable.
Any large scale microservice system needs tools to automatically gather all the necessary metrics and display them in a format consumable to humans.

> Systems are as good as the people who designed it.

Systems fails and that should be expected.
But it should also self recover. How you ask? Not with a help from humans.

> With any advanced automation the weakest link is always the human.

Creating a self healing system requires it to be observable.
To make the platform observable you need monitoring.
Monitoring then should be a priority not an afterthought.

Humans should only be in the loop when something goes critically wrong.
Humans job should not only be fixing the problems but primarily making sure those problems never occur again or would get fixed automatically next time.
This is why there is a need for "platform engineers" (or SRE, however we want to call them). 
Those are either system administrators who can code or coders who knows system administration.  

There is this one twisted interpretation of DevOps where the premise is you could get "rid" of system administrators and be left with only developers who would manage services in production.
That's never gonna happen.  
Most developers don't care and does not want to learn about system administration. 
Just search for "devops engineer" on any job searching portal to see for yourself how many companies struggle to find them.

The opposite of an observable system is a "black box", where the only thing we can see are the inputs and outputs (or a lack therof).
In this very entertaining https://www.youtube.com/watch?v=30jNsCVLpAE[talk] Bryan Cantrill talks about the art of debugability:

> The art of debugging isn't to guess the answer - it is to be able to ask the right questions to know how to answer them. 
> Answered questions are facts, not hyphotesis.

Making platform observable is a hard and underappriciated work.
When a deployment is a non-event nobody congratulates the people behind it.

In my opinion successfully pulling out microservices architecture requires putting more effort on the platform itself then on the services running on it.
Companies needs to realize they are creating a platform first and the services running on it are the afterthought.

