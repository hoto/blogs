---
title: Microservices failures: Ignoring platform complexity
date: "2019-05-18"
---

== Microservices failures: Ignoring platform complexity

I think there is a common misconception about microservices out there.

The popular opinion is that microservices supposed to be the cure for all of our problems.
But I say that by themselves they can only cure half of the problems.
To cure the other half you are suppose to combine microservices with the DevBizSecDbaQaOps practices and transform your company into the ultimate buzzword producing killing machine gun and achieve Continuous Deployment nirvana.

But seriously.

Countless of blogs promised us that microservices will be the the ultimate cure.
They were supposed to enable companies to perform gazillion deployments per day, scale the system up to infinity, reduce the codebase complexity and save money at the same time.
But few asked what is the price for all of this?

Many forgets that microservices architecture shifts the complexity from the code to the platform.

== Disclaimer

I would like to point here out that, if you haven't noticed already, this post are my own opinions on the subject.
I have created my own models in order to understand microservices architecture, especially in the context of Continuous Deployment.
Those models are full of holes and may be completely different than what's accepted as the industry standard.

== Complexity

> Complex - many parts with intricate arrangements, organized (easy) or disorganized (hard)

> Complicated - hard to understand regardless of the complexity

Sometimes things may seem complicated at first but will become complex with enough familiarity (think driving a car).

We can describe any software system on how complicated and complex it is.
System can be made less complicated and complex by removing pieces of it.
When splitting a system into pieces it may become less complicated but the complexity usually increases.

Thankfully splitting can be done in such a way that we end up with an "organized" complexity which is easy to understand.
But that, in my opinion, is an art and requires not only expertise but a ton of experience.

When going from a monolith to microservices we split services reducing their complexity but the complexity does not disappear into thin air, it is transferred onto the platform.

Because designing a platform for distributed systems on a large scale is a pretty new concept many companies don't realize they have to change tactics and fail miserably when giving in the microservices hype.

== Platform definition

Software which is not a shelfware will eventually end up running on a platform.
Platform is all the things which enables the software to run and perform its duty.
When platform fails your software fails.  

My definition of a software platform includes:

* *Platform topology* - existing infrastructure
* *Services* - executable units of software
** *Core services* - executables required to fulfil business needs
** *Support services* - executables monitoring core services and the platform itself
** *Service Orchestration* - executables assigning which service goes where, also responsible for scaling and destroying services
* *Service Configuration* - configuration which changes depending on the environment
* *Service Secrets* - configuration which should not be source controlled
* *Service Discovery* - detection of services
* *Service Mesh* - delivery of requests through a topology

Additionally we have things which describes the platform setup and deployment procedures:

* *Platform definition* - procedures (hopefuly in code) describing in details the infrastructure
* *Deployment pipelines* - procedures (hopefully in code) describing the deployment process

Every platform is alive, evolving and has a lifecycle:

*Day 1*

There is nothing.
Infrastructure gets created and software gets deployment for the first time.

*Day 2+*

Infrastructure gets removed, updated or extended.
Configuration changes.
Software crashes.
Software gets redeployed.
Deployment fails and needs to be rollbacked.

Really difficult problems arise not on day 1, but on day 2 onwards.
If you don't plan and think ahead on the first day you may be in a need one day to scrap your whole platform and go back to the drawing board.

You can see that there is a lot of things to go wrong here.
What's inside the boxes (services) is irrelevant, the thing which handles the boxes should be the main focus of attention.
If the company thinks that the services are more important then the platform then I would recommend to stay with monoliths.

Platform is like a free puppy.
Upfront cost may be zero but when puppy grows and multiplies the maintenance costs grows exponentially.
Eventually a badly designed and poorly maintained platform will behave in the most unexpected ways up to a point where instead of fixing the current one it's cheaper to just create a new one.

== Platform as an afterthought

There are two cases when starting with microservices architecture:

1. Starting fresh - what we call a greenfield project.
2. Migrating from a monolith by extracting bits from it.

Unfortunately both cases usually suffer from the same symptoms:

> Platform design is often an afterthought.

== Common oversights

> "Some people change their ways when they see the light; others when they feel the heat."

In my opinion the most common oversights when dealing with microservices are:

=== 1. Lack of monitoring.

Observability has to be built into the services and the platform from the very beginning.
Don't make a mistake going into production and then worry about observability, it may be too late.

SLIs, SLAs and SLOs, which boils down to https://cloud.google.com/blog/products/gcp/sre-fundamentals-slis-slas-and-slos[availability],  should be agreed up front and monitored.
To monitor those values you need observability.

Often there is a question who will be looking at the monitoring and my answer would be to ask this:
Who cares about not breaking the SLA and what happens when it's broken?

If the answer is "nobody" and "nothing" then you don't need monitoring in the first place because nobody cares if the system is working or not.

But if there is a penalty to break the SLA then somebody's job, or even the whole company, is on the line.

> "People are not afraid of failure, they are afraid of blame."

=== 2. Wrong tools for alerts (or no alerting).
=== 3. Making artifacts mutable.
=== 4. Not following the https://12factor.net/["twelve factors"] rules.
=== 5. Designing pipelines with no automated rollback strategy.
=== 6. Not changing the tools when scale changes.

I was on a project once where the tool for orchestrating services was very primitive.
That tool didn't know about the capacity of the platform. Service assignment was done manually.
You can imagine that would work very well for a small platform with little to no load.

=== 7. Not using a Service Mesh

== Monitoring, observability and debuggability

Monitoring is gathering and displaying data so it can be analyzed.  
To monitor a system it must be observable.

> If you are observable I can understand you.

The tools and techniques needed to analyze a system composed of couple services vs hundredths of services are vastly different.
Where one can manage to manually gather and sift through metrics for few services, doing so for dozens is not sustainable.
Any large scale microservice system needs tools to automatically gather all the necessary metrics and display them in a format consumable to humans.

> Systems are as good as the people who designed it.

Systems fails and that should be expected.
But it should also self recover. How you ask? Not with a help from humans.

> With any advanced automation the weakest link is always the human.

Creating a self healing system requires it to be observable.
To make the platform observable you need monitoring.
Monitoring then should be a priority not an afterthought.

Humans should only be in the loop when something goes critically wrong.
Humans job should not only be fixing the problems but primarily making sure those problems never occur again or would get fixed automatically next time.
This is why there is a need for "platform engineers" (or SRE, however we want to call them). 
Those are either system administrators who can code or coders who knows system administration.  

There is this one twisted interpretation of DevOps where the premise is you could get "rid" of system administrators and be left with only developers who would manage services in production.
That's never gonna happen.  
Most developers don't care and does not want to learn about system administration. 
Just search for "devops engineer" on any job searching portal to see for yourself how many companies struggle to find them.
Also from the job descriptions you can easily tell if a company treats it's platform seriously.

The opposite of an observable system is a "black box", where the only thing we can see are the inputs and outputs (or a lack thereof).
In this very entertaining https://www.youtube.com/watch?v=30jNsCVLpAE[talk] Bryan Cantrill talks about the art of debuggability:

> The art of debugging isn't to guess the answer - it is to be able to ask the right questions to know how to answer them. 
> Answered questions are facts, not hypothesis.

Making platform observable is a hard and under appreciated work.
When a deployment is a non-event nobody congratulates the people behind it.

In my opinion successfully pulling out microservices architecture requires putting more effort on the platform itself then on the services running on it.
Companies needs to realize they are creating a platform first and the services running on it are the afterthought.

