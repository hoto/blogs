//---
//title: Microservices on a budget: ignoring platform complexity
//date: "2019-05-18"
//---

== Microservices failures: ignoring platform complexity

I think there is a common misconception about microservices out there.

The popular opinion is that microservices supposed to be the cure for all of our problems.  
But I say that by themselves they can cure only half of our problems.  
You are suppose to combine them with the latest DevBizSecDbaQaOps practices in order to transform your company into the ultimate buzzword producing killing machinegun and achievie Continous Deployment nirvana.

But seriously.

Countless of blogs promised us that microservices will be the the ultimate cure.
They were supposed to enable companies to perform gazzilions deployments per day, scale the system up to infinity while saving money and reducing the codebase complexity at the same time.  
But few asked what is the price for all of this?

Many forgets that microservices architecture shifts the complexity from code to the platform.

== Complexity

> Complexity - many parts with intricate arrangements, organized (easy) or disorganized (hard)  

> Complicated - hard to understand regardless of the complexity

We can describe any software system on how complicated and complex it is.  
System can be made less complicated and complex by removing pieces of it.  
When splitting a system into pieces it may become less complicated but the complexity will increase. 
Thanfully splitting can be done in such a way that we end up with an organized complexity which is easy to understand.  

Microservices may reduce complexity from the code but just the opposite happens to the platform.  

== Platform definition

If you create software which is not a shelfware it will eventually end up running on a platform.   
It's all the things which enables the software to perform.  
When platform fails your software fails.  

My definition of any software platform includes:

* *Platform topology* - existing infrastructure
* *Services* - executable units of software
  - *Core services* - executables required to fulfil business needs
  - *Support services* - executables monitoring core services and the platform itself
  - *Service Orchestration* - executables assigning which service goes where, also responsible for scalling and destroying services
* *Service Configuration* - configuration which changes depending on the environment
* *Service Secrets* - configuration which should not be source controlled
* *Service Discovery* - detection of services
* *Service Mesh* - delivery of requests through a topology

Additionally we have things which describes the platform setup and deployment procedures:

* *Platform definition* - procedures (hopefuly in code) describing in details the infrastructure
* *Deployment pipelines* - procedures (hopefully in code) describing the deployment process

Every platform has a lifecycle:

* Day 1

  There is nothing.
  Infrastructure gets created and software gets deployment for the first time.

* Day 2+

  Infrastructure gets extended or updated.
  Software gets redeployed.

Project crippling problems arise not on day 1, but on day 2 onwards.
Eventually a badly designed and poorly maintaned platform will behave oddly in the most unexpected ways up to a point where instead of fixing the problems it's cheaper to create a new platform.

== Monitoring

== Alarms

== Scalling

== Platform

First I would like to point out that if you haven't noticed yet this post is my own private opinion on the subject.

There are two cases when starting with microservices architecture:

1. Starting fresh - what we call a greenfield project.
2. Migrating from a monolith by extracting bits from it.

Unfortunally both cases usually suffer from the same symptoms:

> Platform is an afterthought.

== No rewards for paying attention to details

When starting fresh everybody on the project is hyped about microservices.

== Devil is in the details

When starting fresh the core funcionality does not exist yet (obviously) and it will get distributed across many services - that's why we call it a distributed system.

The most common mistakes when starting a distributed system from scratch that I recognize are:

1. 

== Monitoring, observability and debugability

Monitoring is gathering and displaying data so it can be analyzed.  
To monitor a system it must be observable.

> If you are observable I can understand you.

The tools and techniques needed to analyze a system composed of couple services vs hundredths of services are vastly different.
Where one can manage to manually gather and sift through metrics for few services, doing so for dozens is not sustainable.
Any large scale microservice system needs tools to automatically gather all the necessary metrics and display them in a format consumable to humans.

> Systems are as good as the people who designed it.

Systems fails and that should be expected.
But it should also self recover. How you ask? Not with a help from humans.

> With any advanced automation the weakest link is always the human.

Creating a self healing system requires it's observable.
To make the platform observable you need monitoring.
Monitoring is usually an afterthough on a project when it should be the priority.

Humans should only be in the loop when something goes critically wrong.
Humans main job should not only be fixing the problems but primarly making sure those problems never occur again or will be fixed automatically next time.
This is why there is a need for "platform engineers" (or SRE, however we want to call them). 
Those are either system administrators who can code or coders who knows system administration.  

There is this one twisted interpretation of DevOps where the premise is you could get "rid" of system administrators and be left with only developers who would manage services in production.
That's never gonna happen.  
Most developers don't care and does not want to learn about system administration. 
Just search for "devops engineer" on any job searching portal to see for yourself how many companies struggle to find them.

The opposite of an observable system is a "black box", where the only thing we can see are the inputs and outputs (or a lack therof).
In this very entertaining https://www.youtube.com/watch?v=30jNsCVLpAE[talk] Bryan Cantrill talks about the art of debugability:

> The art of debugging isn't to guess the answer - it is to be able to ask the right questions to know how to answer them. 
> Answered questions are facts, not hyphotesis.

Making platform observable is a hard and underappriciated work.
When a deployment is a non-event nobody congratulates the people behind it.

In my opinion successfully pulling out microservices architecture requires putting more effort on the platform itself then on the services running on it.
Companies needs to realize they are creating a platform first and the services running on it are the afterthought.

